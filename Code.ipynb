{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba17b2b3-f5cc-4885-916b-c3b9c060afb6",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ba17b2b3-f5cc-4885-916b-c3b9c060afb6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8770f853-3ea2-4d09-ba23-eb7fdee8df3e",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8770f853-3ea2-4d09-ba23-eb7fdee8df3e"
      },
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "exchange_rate_df = pd.read_csv('Exchange Rate.csv')\n",
        "export_value_df = pd.read_csv('Export Value.csv')\n",
        "fdi_inflow_df = pd.read_csv('FDI Inflow.csv')\n",
        "fpi_df = pd.read_csv('FPI.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dfa15cc-e5cd-4751-bc9d-021dedcf05af",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2dfa15cc-e5cd-4751-bc9d-021dedcf05af"
      },
      "outputs": [],
      "source": [
        "# Drop unnecessary columns\n",
        "exchange_rate_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "export_value_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "fdi_inflow_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "fpi_df.drop(columns=['Unnamed: 0'], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b40bf2d-1197-454f-89c6-4621f9588bde",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3b40bf2d-1197-454f-89c6-4621f9588bde"
      },
      "outputs": [],
      "source": [
        "# converting exchange rate to yearly data using the average\n",
        "exchange_rate = exchange_rate_df.groupby(['Area', 'Year']).mean().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f42c75f3-1602-4383-ac49-177f71cc5cf1",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f42c75f3-1602-4383-ac49-177f71cc5cf1",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "exchange_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c614d50-b706-4730-a96a-02e87486dbff",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1c614d50-b706-4730-a96a-02e87486dbff",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# converting fpi to yearly data using the average\n",
        "fpi = fpi_df.groupby(['Area', 'Year']).mean().reset_index()\n",
        "fpi.head(205)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e10e5f68-ef65-4116-9ffc-6291d0d477a2",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e10e5f68-ef65-4116-9ffc-6291d0d477a2",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#making df an excel file\n",
        "exchange_rate.to_csv(\"Exchange Rate yearly.csv\")\n",
        "fpi.to_csv(\"FPI yearly.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d664f64-e21a-46be-b2ef-b95e51f85895",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6d664f64-e21a-46be-b2ef-b95e51f85895"
      },
      "outputs": [],
      "source": [
        "# Merging files\n",
        "data = export_value_df.merge(exchange_rate, on=['Area', 'Year'], how='left') \\\n",
        "                      .merge(fdi_inflow_df, on=['Area', 'Year'], how='left') \\\n",
        "                      .merge(fpi, on=['Area', 'Year'], how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a7f939e-9f1e-4732-8211-9df2675e907d",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4a7f939e-9f1e-4732-8211-9df2675e907d",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21958b5e-543d-4c1d-8115-d9256624546f",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "21958b5e-543d-4c1d-8115-d9256624546f",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "data.to_csv(\"merged data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7b900c2-cb61-414f-9484-26d2442952af",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d7b900c2-cb61-414f-9484-26d2442952af",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90aa6e5a-e6ab-4acd-a34a-cd6c6089f434",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "90aa6e5a-e6ab-4acd-a34a-cd6c6089f434"
      },
      "outputs": [],
      "source": [
        "# Renaming column\n",
        "data = data.rename(columns={'FDI_USDm': 'FDI Inflow'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bbdf3eb-94ba-435b-bed8-0fb1af6a07b0",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2bbdf3eb-94ba-435b-bed8-0fb1af6a07b0"
      },
      "outputs": [],
      "source": [
        "# Checking for entire duplicate rows\n",
        "duplicate_rows = data.duplicated()\n",
        "print(\"Number of duplicate rows:\", duplicate_rows.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40fef237-327f-4e38-ac70-5b503f8ac1de",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "40fef237-327f-4e38-ac70-5b503f8ac1de"
      },
      "outputs": [],
      "source": [
        "# checking for missing values\n",
        "missing_counts = data.isnull().sum()\n",
        "print(missing_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2289b8f5-8ded-44e4-8fbe-44d2de6ddcb2",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2289b8f5-8ded-44e4-8fbe-44d2de6ddcb2",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Filtering out columns that have no missing values\n",
        "missing_counts = missing_counts[missing_counts > 0]\n",
        "\n",
        "# Creating a bar plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "missing_counts.plot(kind='bar')\n",
        "plt.title('Figure . Missing Values Count by Column')\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Number of Missing Values')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9ed7c7a-8abe-4a2d-98a0-dd9cd24c6c35",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f9ed7c7a-8abe-4a2d-98a0-dd9cd24c6c35"
      },
      "outputs": [],
      "source": [
        "# Get the initial shape of the dataset\n",
        "initial_shape = data.shape\n",
        "print(f'Initial dataset shape: {initial_shape}')\n",
        "\n",
        "# Dropping rows with any missing values\n",
        "df = data.dropna()\n",
        "\n",
        "# Getting the shape of the dataset after dropping missing values\n",
        "final_shape = df.shape\n",
        "print(f'Final dataset shape after dropping missing values: {final_shape}')\n",
        "\n",
        "# Calculating the number of rows dropped\n",
        "rows_dropped = initial_shape[0] - final_shape[0]\n",
        "print(f'Number of rows dropped: {rows_dropped}')\n",
        "\n",
        "# Calculating the percentage of data retained\n",
        "percentage_retained = (final_shape[0] / initial_shape[0]) * 100\n",
        "print(f'Percentage of data retained: {percentage_retained:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7b33256-b655-473c-b740-1042161a88d4",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e7b33256-b655-473c-b740-1042161a88d4"
      },
      "outputs": [],
      "source": [
        "df = df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94810d8b-26db-46c0-9fb7-05e6c8476d98",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "94810d8b-26db-46c0-9fb7-05e6c8476d98"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3a95887-4dec-4b27-83ba-418a84ebf352",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f3a95887-4dec-4b27-83ba-418a84ebf352",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad0398b2-1cef-4aae-9064-4da66101c5a7",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ad0398b2-1cef-4aae-9064-4da66101c5a7"
      },
      "outputs": [],
      "source": [
        "# displaying format to float, 2 decimal places\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3ba4d51-b893-411a-af29-4d6a79a83087",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "c3ba4d51-b893-411a-af29-4d6a79a83087",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df.groupby('Item').count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a420451d-3f31-4a11-b8b1-eea1ea0362b9",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a420451d-3f31-4a11-b8b1-eea1ea0362b9",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Checking highest export value from highest to lowest by region\n",
        "ranking_export_value=df.groupby(['Area'],sort=True)['Export Value'].sum().nlargest(10)\n",
        "\n",
        "# Creating a bar plot\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.barplot(x=ranking_export_value.values, y=ranking_export_value.index, palette=\"viridis\")\n",
        "plt.title('Top 10 Areas by Export Value', fontsize=16)\n",
        "plt.xlabel('Export Value', fontsize=14)\n",
        "plt.ylabel('Area', fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd0cbd52-d822-4a06-8823-e28cd5e3f7a3",
      "metadata": {
        "id": "fd0cbd52-d822-4a06-8823-e28cd5e3f7a3"
      },
      "source": [
        "The United States has the highest export value in the dataset.\n",
        "Dominated crop in dataset is fruits and vegeables with 4 countries where it is the highest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2339fb4-1d8c-402b-aff2-7a7c6986bf03",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d2339fb4-1d8c-402b-aff2-7a7c6986bf03"
      },
      "outputs": [],
      "source": [
        "#Highest export value by Item and Area\n",
        "top_item_export_value = df.groupby(['Item','Area'],sort=True)['Export Value'].sum().nlargest(10)\n",
        "top_item_export_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09f01b60-d2a5-413b-b274-5c6c7a06a8ed",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "09f01b60-d2a5-413b-b274-5c6c7a06a8ed",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Boxplot of variables\n",
        "plt.figure(figsize = (10,10))\n",
        "\n",
        "plt.subplot(3,2,1)\n",
        "sns.boxplot(data= df['Year'])\n",
        "plt.title('Year')\n",
        "\n",
        "\n",
        "plt.subplot(3,2,2)\n",
        "sns.boxplot(data= df['Export Value'])\n",
        "plt.title('Export Value')\n",
        "\n",
        "plt.subplot(3,2,3)\n",
        "sns.boxplot(data= df['Exchange Rate'])\n",
        "plt.title('Exchange Rate')\n",
        "\n",
        "plt.subplot(3,2,4)\n",
        "sns.boxplot(data= df['FDI Inflow'])\n",
        "plt.title('FDI Inflow')\n",
        "\n",
        "plt.subplot(3,2,5)\n",
        "sns.boxplot(data= df['FPI'])\n",
        "plt.title('FPI')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2ab34cf-f69f-4ed6-b498-9eaa04528ddf",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e2ab34cf-f69f-4ed6-b498-9eaa04528ddf",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "411ab587-dc18-4f3f-bd84-42463de5bed0",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "411ab587-dc18-4f3f-bd84-42463de5bed0",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# correlation matrix between variables\n",
        "num_cor = df.select_dtypes(['int64','float64']).corr()\n",
        "sns.heatmap(num_cor,cmap = 'YlGnBu',annot = True)\n",
        "plt.title('Heatmap')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0402d1e-afd0-4f92-8289-e19dce0e808f",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f0402d1e-afd0-4f92-8289-e19dce0e808f"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19ab84be-14b2-45fd-911b-7d85f4082466",
      "metadata": {
        "id": "19ab84be-14b2-45fd-911b-7d85f4082466"
      },
      "source": [
        "Can be seen that there is mostly a negative correlation between variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eb07912-a3aa-4e18-8455-702aa464564a",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4eb07912-a3aa-4e18-8455-702aa464564a",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Function to create labels (export value at t+3)\n",
        "def create_labels(df, column_name, lead, groupby_columns=['Area']):\n",
        "    df[f'{column_name}_t+{lead}'] = df.groupby(groupby_columns)[column_name].shift(-lead)\n",
        "    return df\n",
        "\n",
        "# Creating label for export value three years ahead (t+3)\n",
        "df = create_labels(df, 'Export Value', lead=3)\n",
        "\n",
        "# Dropping rows with missing values in the label column\n",
        "df.dropna(subset=['Export Value_t+3'], inplace=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48ed65b1-2656-4f5a-9fda-a18f3fc4e461",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "48ed65b1-2656-4f5a-9fda-a18f3fc4e461"
      },
      "outputs": [],
      "source": [
        "# Creating lagged features for the target and independent variables\n",
        "\n",
        "for feature in ['Export Value', 'Exchange Rate', 'FDI Inflow', 'FPI']:\n",
        "    for lag in range(1, 4):\n",
        "        df[f'{feature}_Lag_{lag}'] = df.groupby('Area')[feature].shift(lag)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb7c646c-3ab5-4d0f-aaeb-5a86db33198a",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cb7c646c-3ab5-4d0f-aaeb-5a86db33198a",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f341dc29-8f93-43c5-9838-c257b50230e2",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f341dc29-8f93-43c5-9838-c257b50230e2",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# dropping rows with missing values\n",
        "df = df.dropna()\n",
        "df = df.reset_index(drop=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13904823-2630-4ca1-a471-51fdd985b56c",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "13904823-2630-4ca1-a471-51fdd985b56c",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Applying one-hot encoding to categorical variables\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "df = pd.get_dummies(df,columns=['Item','Area'], dtype=float, drop_first=True)\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5c7a32e",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d5c7a32e"
      },
      "outputs": [],
      "source": [
        "# Recoding labels into two classes\n",
        "threshold = df['Export Value_t+3'].median()\n",
        "\n",
        "def recode_labels(value):\n",
        "    if value < threshold:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "df['Export Value_t+3_label'] = df['Export Value_t+3'].apply(recode_labels)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ffc8e14-92e9-4bc6-92c7-d938aa8b0b2a",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2ffc8e14-92e9-4bc6-92c7-d938aa8b0b2a"
      },
      "outputs": [],
      "source": [
        "# features and target variable\n",
        "X = df.drop(columns=['Export Value_t+3', 'Export Value', 'Year', 'Exchange Rate', 'FDI Inflow', 'FPI']) # features\n",
        "y = df['Export Value_t+3_label'] # target"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a0ba95f-0320-4586-9e65-42c42fdc4cac",
      "metadata": {
        "id": "7a0ba95f-0320-4586-9e65-42c42fdc4cac"
      },
      "source": [
        "Export Value has been dropped from the 'X' and stored in 'y' as it is the target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "552ed4c9-58b1-48b4-93d8-ec8962ebdb82",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "552ed4c9-58b1-48b4-93d8-ec8962ebdb82"
      },
      "outputs": [],
      "source": [
        "# Shows that there is 44180  and 195 columns\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "262716e9-ce80-4cac-b246-4543d34b1dd4",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "262716e9-ce80-4cac-b246-4543d34b1dd4"
      },
      "outputs": [],
      "source": [
        "# Scaling/Normalization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler=StandardScaler()\n",
        "X_normalized=scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea88b3ef-5e6e-4260-9942-86e516ce0015",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ea88b3ef-5e6e-4260-9942-86e516ce0015"
      },
      "outputs": [],
      "source": [
        "# Randomly Splitting data into train/test using 80:20 split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_rem, X_test, y_rem, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "# Further splitting the remaining data 80:20 into training (64%) and validation sets (16%)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_rem, y_rem, test_size=0.2, random_state=42, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sC1RbEb0SUf1",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sC1RbEb0SUf1"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of instances in training set: {X_train.shape[0]}\")\n",
        "print(f\"Number of instances in test set: {X_test.shape[0]}\")\n",
        "print(f\"Number of instances in validation set:{X_val.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bf65cea-5170-4849-aa8c-3d808a084f2b",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2bf65cea-5170-4849-aa8c-3d808a084f2b",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Define the function to plot label distribution\n",
        "def plot_label_distr(labels, plot_title):\n",
        "    plt.figure()\n",
        "    the_bin_centres = np.unique(labels)\n",
        "    plt.hist(labels, bins=the_bin_centres.shape[0], range=(the_bin_centres[0]-0.5, the_bin_centres[-1]+0.5))\n",
        "    plt.xticks(the_bin_centres)\n",
        "    plt.title(plot_title)\n",
        "    plt.show()\n",
        "    print('\\n')\n",
        "\n",
        "# Plotting the label distributions for training, validation, and test sets\n",
        "plot_label_distr(y_train, 'Class frequencies - Training set')\n",
        "plot_label_distr(y_val, 'Class frequencies - Validation set')\n",
        "plot_label_distr(y_test, 'Class frequencies - Test set')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "p0o9sW2nfQ3A",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "p0o9sW2nfQ3A"
      },
      "outputs": [],
      "source": [
        "##"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1uvAbXnwfWwH",
      "metadata": {
        "id": "1uvAbXnwfWwH"
      },
      "source": [
        "# Multilayer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cCsbwLw_fmnH",
      "metadata": {
        "id": "cCsbwLw_fmnH"
      },
      "source": [
        "Creating methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3d2fe03",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a3d2fe03",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import random\n",
        "import numpy\n",
        "\n",
        "# Ensuring reproducibility\n",
        "random_seed = 42\n",
        "random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "numpy.random.seed(random_seed)\n",
        "\n",
        "## Creating the network structure\n",
        "class three_layer_MLP(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_size,\n",
        "                 hidden_layer_sizes,\n",
        "                 output_size):\n",
        "        super().__init__()\n",
        "        self.hidden_l1 = nn.Linear(input_size, hidden_layer_sizes[0])\n",
        "        self.hidden_l2 = nn.Linear(hidden_layer_sizes[0], hidden_layer_sizes[1])\n",
        "        self.output_l3 = nn.Linear(hidden_layer_sizes[1], output_size)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        out = self.hidden_l1(inputs)\n",
        "        out = self.hidden_l2(out)\n",
        "        out = self.output_l3(out)\n",
        "        out = torch.softmax(out, 1)\n",
        "        return out\n",
        "\n",
        "# A method for computing performance metrics of interest\n",
        "def my_metrics(labels, predictions, show_confusion_matrix=False):\n",
        "\n",
        "    ## First work out which class has been predicted for each data sample. Hint: use argmax\n",
        "    ## Second count how many of these are correctly predicted\n",
        "    ## Finally return the accuracy, i.e. the percentage of samples correctly predicted\n",
        "\n",
        "    predictions_numpy = predictions.detach().numpy()\n",
        "    predicted_classes = numpy.argmax(predictions_numpy, axis=1)\n",
        "\n",
        "\n",
        "    f1_scores = f1_score(labels, predicted_classes, average=None)\n",
        "    acc = accuracy_score(labels, predicted_classes)\n",
        "\n",
        "    if show_confusion_matrix:\n",
        "      print(\"\\n Confusion matrix:\")\n",
        "      confus_mat = confusion_matrix(labels, predicted_classes)\n",
        "      disp = ConfusionMatrixDisplay(confus_mat)\n",
        "      disp.plot()\n",
        "      plt.show()\n",
        "\n",
        "    return f1_scores, acc\n",
        "\n",
        "\n",
        "# A class for managing the data for training the model\n",
        "class MetDataset(Dataset):\n",
        "    def __init__(self, feats, labels):\n",
        "        # Converting features and labels from numpy arrays to PyTorch tensors\n",
        "        self.feats = torch.tensor(feats, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels.values if isinstance(labels, pd.Series) else labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        return self.feats[idx, :], self.labels[idx]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "btJistxxfuzH",
      "metadata": {
        "id": "btJistxxfuzH"
      },
      "source": [
        "Running model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d60e90f",
      "metadata": {
        "id": "9d60e90f"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "# Creating an instance of the MLP network\n",
        "feature_count = X_train.shape[1]\n",
        "hidden_layer_sizes = [10, 10]\n",
        "class_count = 2\n",
        "model = three_layer_MLP(feature_count, hidden_layer_sizes, class_count)\n",
        "\n",
        "\n",
        "# Setting hyperparameters\n",
        "num_epochs = 30\n",
        "learning_rate = 0.05\n",
        "batch_size = 50\n",
        "\n",
        "\n",
        "# Setting up the data loading by batch\n",
        "# With the test and validation sets having only one batch\n",
        "train_set = MetDataset(X_train, y_train)\n",
        "train_dataloader = DataLoader(train_set, batch_size=batch_size)\n",
        "\n",
        "val_set = MetDataset(X_val, y_val)\n",
        "val_dataloader = DataLoader(val_set, batch_size=len(val_set))\n",
        "\n",
        "test_set = MetDataset(X_test, y_test)\n",
        "test_dataloader = DataLoader(test_set, batch_size=len(test_set))\n",
        "\n",
        "\n",
        "\n",
        "# Setting up the SGD optimizer for updating the model weights\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "# Computing cross entropy loss against the training labels\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "best_model_acc = 0\n",
        "losses = []\n",
        "\n",
        "# Iterating over the dataset at two different staages:\n",
        "# 1. Iterating over the batches in the dataset (inner for loop below)\n",
        "# One complete set of iteration through the dataset (i.e. having gone over\n",
        "# all batches in the dataset at least once) = One epoch\n",
        "# 2. Iterating over the specified numeber of epochs (outer for loop below)\n",
        "for epoch in range(0, num_epochs):\n",
        "\n",
        "    # Setting the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    if epoch == 0:  best_model = deepcopy(model)\n",
        "\n",
        "    for batch, (X_train_batch, y_train_batch) in enumerate(train_dataloader):\n",
        "\n",
        "      # Zeroing out the `.grad` buffers,\n",
        "      # otherwise on the backward pass we'll add the\n",
        "      # new gradients to the old ones.\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Computing the forward pass and then the loss\n",
        "      train_pred = model.forward(X_train_batch)\n",
        "      train_loss = loss_function(train_pred, y_train_batch)\n",
        "      train_avg_f1_score, train_acc = my_metrics(y_train_batch, train_pred)\n",
        "\n",
        "      # Computing the model parameters' gradients\n",
        "      # and propagating the loss backwards through the network.\n",
        "      train_loss.backward()\n",
        "\n",
        "      # Updating the model parameters using those gradients\n",
        "      optimizer.step()\n",
        "\n",
        "    # Evaluating on the validation set\n",
        "    model.eval()\n",
        "    for batch, (X_val_batch, y_val_batch) in enumerate(val_dataloader):\n",
        "      val_pred = model.forward(X_val_batch)\n",
        "      val_loss = loss_function(val_pred, y_val_batch)\n",
        "      val_avg_f1_score, val_acc = my_metrics(y_val_batch, val_pred)\n",
        "\n",
        "    if val_acc > best_model_acc:\n",
        "      best_model_acc = val_acc\n",
        "      best_model = deepcopy(model)\n",
        "      print('Found improvement in performance. New model saved.')\n",
        "\n",
        "    # How well the network does on the batches\n",
        "    # is an indication of how well training is progressing\n",
        "    print(\"epoch: {} - train loss: {:.4f} train acc: {:.2f} val loss: {:.4f} val acc: {:.2f}\".format(\n",
        "        epoch,\n",
        "        train_loss.item(),\n",
        "        train_acc,\n",
        "        val_loss.item(),\n",
        "        val_acc ))\n",
        "\n",
        "    losses.append([train_loss.item(), val_loss.item()])\n",
        "\n",
        "model = best_model\n",
        "\n",
        "# Testing model on the test set to get an estimate of its performance.\n",
        "# First set the model to evaluation mode\n",
        "model.eval()\n",
        "data_instance_ids = []\n",
        "true_labels = []\n",
        "model_predictions = []\n",
        "\n",
        "for batch, (X_test_batch, y_test_batch) in enumerate(test_dataloader):\n",
        "  test_pred = model.forward(X_test_batch)\n",
        "  test_f1_scores, test_accuracy = my_metrics(y_test_batch, test_pred, show_confusion_matrix=True)\n",
        "  print(\"\\n test accuracy: {:2.2f}\".format(test_accuracy))\n",
        "  test_pred_numpy = test_pred.detach().numpy()\n",
        "  print('\\n The F1 scores for each of the classes are: '+str(test_f1_scores))\n",
        "\n",
        "  print(\"\\n Loss graph:\")\n",
        "  fig, ax = plt.subplots()\n",
        "  losses = numpy.array(losses)\n",
        "  ax.plot(losses[:, 0], 'b-', label='training loss')\n",
        "  ax.plot(losses[:, 1], 'k-', label='validation loss')\n",
        "  plt.legend(loc='upper right')\n",
        "\n",
        "  instance_ids = range(len(y_test_batch))  # Adjust this if you have specific instance IDs\n",
        "  data_instance_ids.extend(instance_ids)\n",
        "  true_labels.extend(y_test_batch.numpy())\n",
        "  model_predictions.extend(test_pred_numpy)\n",
        "\n",
        "\n",
        "# Saving results to CSV\n",
        "output_df = pd.DataFrame({\n",
        "    'Data Instance ID': data_instance_ids,\n",
        "    'True Label': true_labels,\n",
        "    'Predicted value': model_predictions\n",
        "})\n",
        "\n",
        "output_csv_path = 'model_predictions.csv'\n",
        "output_df.to_csv(output_csv_path, index=False, header=True)\n",
        "\n",
        "print(f\"Predictions saved to {output_csv_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "y_test_batch.size()"
      ],
      "metadata": {
        "id": "lpRHZaiM85C_"
      },
      "id": "lpRHZaiM85C_",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}